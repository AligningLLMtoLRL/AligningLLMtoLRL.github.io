{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7658007,"sourceType":"datasetVersion","datasetId":4465031}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tuning mT5 for Low-resource Languages\n\n## mT5\n\nMultilingual T5 ([mT5](https://github.com/google-research/multilingual-t5)) is a massively multilingual pretrained text-to-text\ntransformer model, trained following a similar recipe as\n[T5](https://github.com/google-research/text-to-text-transfer-transformer). T5 was introduced in the paper [_Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer_](https://arxiv.org/abs/1910.10683). \n\n<img src=\"https://1.bp.blogspot.com/-o4oiOExxq1s/Xk26XPC3haI/AAAAAAAAFU8/NBlvOWB84L0PTYy9TzZBaLf6fwPGJTR0QCLcBGAsYHQ/s1600/image3.gif\" width=\"700\" height=\"300\" />","metadata":{}},{"cell_type":"markdown","source":"## Languages covered\n\nmT5 is pretrained on the [mC4](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual_nights_stay) corpus, covering 101 languages:\n\nAfrikaans, Albanian, Amharic, Arabic, Armenian, Azerbaijani, Basque,\nBelarusian, Bengali, Bulgarian, Burmese, Catalan, Cebuano, Chichewa, Chinese,\nCorsican, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino,\nFinnish, French, Galician, Georgian, German, Greek, Gujarati, Haitian Creole,\nHausa, Hawaiian, Hebrew, Hindi, Hmong, Hungarian, Icelandic, Igbo, Indonesian,\nIrish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish,\nKyrgyz, Lao, Latin, Latvian, Lithuanian, Luxembourgish, Macedonian, Malagasy,\nMalay, Malayalam, Maltese, Maori, Marathi, Mongolian, Nepali, Norwegian,\nPashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Samoan,\nScottish Gaelic, Serbian, Shona, Sindhi, Sinhala, Slovak, Slovenian, Somali,\nSotho, Spanish, Sundanese, **Swahili**, Swedish, Tajik, Tamil, Telugu, Thai,\nTurkish, Ukrainian, Urdu, Uzbek, Vietnamese, Welsh, West Frisian, Xhosa,\nYiddish, Yoruba, Zulu.","metadata":{}},{"cell_type":"markdown","source":"*PS: This notebook is built on Kaggle using ***GPU T4x2*** accelerator and it is prepared based on https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/flan-t5-samsum-summarization.ipynb*","metadata":{}},{"cell_type":"markdown","source":"## Task: Instruct mT5 to summarize Swahili content\n\nWe use [**\"XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages\"**](https://aclanthology.org/2021.findings-acl.413/) to evaluate summarization performance in Swahili","metadata":{}},{"cell_type":"code","source":"# python\n!pip install pytesseract transformers datasets py7zr --upgrade\n!pip install evaluate rouge-score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Only use the 10% of the test split for a fast demonstration of evaluation \nxlsum_swa_test = load_dataset(\"csebuetnlp/xlsum\", \"swahili\", split='test[:10%]')","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:26:24.989562Z","iopub.execute_input":"2024-02-21T11:26:24.990026Z","iopub.status.idle":"2024-02-21T11:26:26.890549Z","shell.execute_reply.started":"2024-02-21T11:26:24.989993Z","shell.execute_reply":"2024-02-21T11:26:26.889519Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"swa_sample = xlsum_swa_test[0]\n\nprint(f\"Text: \\n{swa_sample['text']}\\n---------------\")\nprint(f\"Summary: \\n{swa_sample['summary']}\\n---------------\")","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:26:36.212848Z","iopub.execute_input":"2024-02-21T11:26:36.213771Z","iopub.status.idle":"2024-02-21T11:26:36.219522Z","shell.execute_reply.started":"2024-02-21T11:26:36.213736Z","shell.execute_reply":"2024-02-21T11:26:36.218471Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Text: \nMessi na Mbappe Ingawa kila mwamba ngoma, ngozi huivutia kwake, ukweli ni kuwa, wachezaji wapya wametumia vyema jukwaa lililopatikana Kombe la Dunia Urusi 2018. Baada ya kutawala vinywa vya wengi ndani ya zaidi ya kipindi cha miaka 10, kuondoka kwa Argentina na Ureno Urusi hatua za mchujo, imewanyima Messi na Ronaldo muda zaidi wa kujadiliwa. Kurunzi za soka sasa zimeelekezwa kwa Neymar wa Brazil, Paul Pogba, Kylian Mbappe na Antoine Griezmann wa Ufaransa, Edison Cavani na Luis Suarez wa Uruguay, Harry Kane wa Uingereza na Romelu Lukaku wa Ubelgiji. Orodha ni ndefu, lakini yote yataamuliwa kulingana na mchango wa wachezaji hawa kwa ufanisi wa timu zao. Mfungaji bora kombe la Dunia 2018 Hatua hii inamaanisha kwamba Kuondoka kwa Ureno imekuwa pigo la kipekee kwa Ronaldo, aliyejitahidi kuzidisha mabao yake Urusi kwani ni wazi sasa wenzake watampiku kwenye uwaniaji wa tuzo ya mfungaji bora. Ronaldo, alianza vyema baada ya kuondoka na mpira wa mechi dhidi ya Uhispania alipofunga hat-trick mechi yake ya kwanza. Kipa wa Iran Ali Beiranvand pia alimnyima kwa kushika penalti yake walipokutana. Kwa sasa mbio ni kati ya Lukaku, Harry Kane, Cavani na Mbappe na wengine. Diego Costa wa Uhispania na Cheryshev wa Urusi watapata fursa ya kufunga mabao wawili hao watakapokwaruzana siku ya Jumapili katika uwanja wa Luzhniki, Moscow.\n---------------\nSummary: \nUkurasa mpya umefunguliwa kwa mashabiki wa soka duniani kuhusu ubora wa wachezaji maarufu wa soka.\n---------------\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_id=\"google/mt5-base\"\n\n# Load tokenizer of mt5-base\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmax_input_len = 256\nmax_target_len = 64","metadata":{"execution":{"iopub.status.busy":"2024-02-21T04:11:29.232357Z","iopub.execute_input":"2024-02-21T04:11:29.232898Z","iopub.status.idle":"2024-02-21T04:11:38.279647Z","shell.execute_reply.started":"2024-02-21T04:11:29.232856Z","shell.execute_reply":"2024-02-21T04:11:38.278809Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/376 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5427509118c46c595972b4baafcdb60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/702 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e39800221c4b4b57914b4c605b98f1e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"975f74c56b0d4dbc9aa29c3454329814"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28e3a927fe25408fa831f0e0851a6d79"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess_xlsum(examples, padding=\"max_length\"):\n    inputs = [f'Summarize the follow text:\\n{text}' for text in examples[\"text\"]]\n    \n     # tokenize inputs\n    model_inputs = tokenizer(inputs, max_length=max_input_len, padding=padding, truncation=True)\n\n    # Tokenize targets with the `text_target` keyword argument\n    labels = tokenizer(text_target=examples[\"summary\"], max_length=max_target_len, padding=padding, truncation=True)\n    \n    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n    # padding in the loss.\n    if padding == \"max_length\":\n        labels[\"input_ids\"] = [\n            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n        ]\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_xlsum_swa_test = xlsum_swa_test.map(preprocess_xlsum, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T04:11:39.512499Z","iopub.execute_input":"2024-02-21T04:11:39.513792Z","iopub.status.idle":"2024-02-21T04:11:39.730155Z","shell.execute_reply.started":"2024-02-21T04:11:39.513753Z","shell.execute_reply":"2024-02-21T04:11:39.729266Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/99 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dcb06cc5f064badb648c5bfc12c4d9c"}},"metadata":{}}]},{"cell_type":"code","source":"import evaluate\nimport nltk\nimport numpy as np\nfrom nltk.tokenize import sent_tokenize\nnltk.download(\"punkt\")\n\n# Metric\nmetric = evaluate.load(\"rouge\")\n\n# helper function to postprocess text\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [label.strip() for label in labels]\n\n    # rougeLSum expects newline after each sentence\n    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n\n    return preds, labels\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    # Replace -100 in the predictions as we can't decode them.    \n    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Some simple post-processing\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    result = {k: round(v * 100, 4) for k, v in result.items()}\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-02-21T04:11:41.902304Z","iopub.execute_input":"2024-02-21T04:11:41.902966Z","iopub.status.idle":"2024-02-21T04:11:57.887935Z","shell.execute_reply.started":"2024-02-21T04:11:41.902929Z","shell.execute_reply":"2024-02-21T04:11:57.886881Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-02-21 04:11:44.017316: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-21 04:11:44.017440: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-21 04:11:44.187700: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7613a0eaea2244b3b87bced5acf511dc"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM\n\n# load model from the hub\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T04:11:57.889438Z","iopub.execute_input":"2024-02-21T04:11:57.889988Z","iopub.status.idle":"2024-02-21T04:12:18.641127Z","shell.execute_reply.started":"2024-02-21T04:11:57.889960Z","shell.execute_reply":"2024-02-21T04:12:18.640373Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8ac1a798a564a7b8853e5009ad53622"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4ac500013eb437f9285395244e5f697"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Q1: Can we leverage an existing high quality instruction dataset for this task which are commonly only available **in English**","metadata":{}},{"cell_type":"code","source":"dolly_english = load_dataset(\"databricks/databricks-dolly-15k\")\n\ndef preprocess_dolly(examples, padding=\"max_length\"):\n    inputs = []\n    targets = []\n    for instruction, context in zip(examples[\"instruction\"], examples[\"context\"]):\n        if len(context) > 0:\n          inputs.append(f'{instruction}\\nContext: {context}')\n        else:\n          inputs.append(instruction)\n    \n    # tokenize inputs\n    model_inputs = tokenizer(inputs, max_length=max_input_len, padding=padding, truncation=True)\n\n    # Tokenize targets with the `text_target` keyword argument\n    labels = tokenizer(text_target=examples[\"response\"], max_length=max_target_len, padding=padding, truncation=True)\n    \n    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n    # padding in the loss.\n    if padding == \"max_length\":\n        labels[\"input_ids\"] = [\n            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n        ]\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_dolly_english = dolly_english.map(preprocess_dolly, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T04:12:27.529994Z","iopub.execute_input":"2024-02-21T04:12:27.530815Z","iopub.status.idle":"2024-02-21T04:12:40.713946Z","shell.execute_reply.started":"2024-02-21T04:12:27.530786Z","shell.execute_reply":"2024-02-21T04:12:40.712925Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/8.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c099159f31a462aada4211367efcd41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/13.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90362e56f6ad469ab010ef0352f07bd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63f5014ac080485cb92e6c8f225158e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/15011 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40b151ba38fc4325b032f8b96c8dd296"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\n# we want to ignore tokenizer pad token in the loss\nlabel_pad_token_id = -100\n# Data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    label_pad_token_id=label_pad_token_id,\n    pad_to_multiple_of=8\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T04:13:15.889527Z","iopub.execute_input":"2024-02-21T04:13:15.890225Z","iopub.status.idle":"2024-02-21T04:13:15.894709Z","shell.execute_reply.started":"2024-02-21T04:13:15.890196Z","shell.execute_reply":"2024-02-21T04:13:15.893749Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import HfFolder\nfrom transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\ntrain_batch_size = 2\neval_batch_size = 8\n\n# Hugging Face repository id\neng_model_id = f\"mt5_base_eng_dolly\"\n\n# Create Trainer instance\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=Seq2SeqTrainingArguments(\n        output_dir=eng_model_id,\n        per_device_train_batch_size=train_batch_size,\n        per_device_eval_batch_size=eval_batch_size,\ngradient_accumulation_steps=4,\n        predict_with_generate=True,\n        #fp16=True, # Overflows with fp16\n        learning_rate=1e-3,\n        max_steps=1000,\n        # logging & evaluation strategies\n        logging_strategy=\"steps\",\n        logging_steps=250,\n        evaluation_strategy=\"no\",\n        save_strategy=\"no\",\n        load_best_model_at_end=True,\n        generation_max_length=max_target_len,\n        report_to=\"none\",\n        push_to_hub=False,\n    ),\n    data_collator=data_collator,\n    train_dataset=tokenized_dolly_english['train'],\n    eval_dataset=tokenized_xlsum_swa_test,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T04:13:45.933245Z","iopub.execute_input":"2024-02-21T04:13:45.934101Z","iopub.status.idle":"2024-02-21T04:13:46.815537Z","shell.execute_reply.started":"2024-02-21T04:13:45.934070Z","shell.execute_reply":"2024-02-21T04:13:46.814549Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Start training\ntrainer_en.train()\n\n# Save the model\ntrainer_en.save_model()","metadata":{"execution":{"iopub.status.busy":"2024-02-21T04:14:06.535741Z","iopub.execute_input":"2024-02-21T04:14:06.536467Z","iopub.status.idle":"2024-02-21T04:28:59.259045Z","shell.execute_reply.started":"2024-02-21T04:14:06.536436Z","shell.execute_reply":"2024-02-21T04:28:59.258040Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 14:42, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>250</td>\n      <td>4.920000</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>3.649200</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>3.426600</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>3.267900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluate the fine-tuned model\ntrainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-02-20T17:31:22.100159Z","iopub.execute_input":"2024-02-20T17:31:22.100604Z","iopub.status.idle":"2024-02-20T17:31:38.959167Z","shell.execute_reply.started":"2024-02-20T17:31:22.100566Z","shell.execute_reply":"2024-02-20T17:31:38.958112Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 5.133183479309082,\n 'eval_rouge1': 11.5638,\n 'eval_rouge2': 4.4385,\n 'eval_rougeL': 10.0082,\n 'eval_rougeLsum': 10.0471,\n 'eval_gen_len': 46.17171717171717,\n 'eval_runtime': 16.8475,\n 'eval_samples_per_second': 5.876,\n 'eval_steps_per_second': 0.415,\n 'epoch': 0.27}"},"metadata":{}}]},{"cell_type":"code","source":"import torch\n\n# Free memory for the second training \ndel model\ndel trainer_en\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-02-21T04:31:12.388093Z","iopub.execute_input":"2024-02-21T04:31:12.388433Z","iopub.status.idle":"2024-02-21T04:31:12.869863Z","shell.execute_reply.started":"2024-02-21T04:31:12.388407Z","shell.execute_reply":"2024-02-21T04:31:12.868796Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# English example\nen_context = \"Gundogan, 26, told BBC Sport he can see the finishing line after tearing cruciate knee ligaments in December, but will not rush his return. The German missed the 2014 World Cup following back surgery that kept him out for a year, and sat out Euro 2016 because of a dislocated kneecap. He said that it is heavy mentally to accept that. Gundogan will not be fit for the start of the Premier League season at Brighton on 12 August but said his recovery time is now being measured in weeks rather than months. Gundogan made 15 appearances and scored five goals in his debut season for City following his £20m move from Borussia Dortmund. He is eager to get on the field again and was impressed at the club's 4-1 win over Real Madrid in a pre-season game in Los Angeles on Wednesday. Manager Pep Guardiola has made five new signings already this summer and continues to have an interest in Arsenal forward Alexis Sanchez and Monaco's Kylian Mbappe. Gundogan said that we felt that last year as well but it was a completely new experience for all of us. We know the Premier League a bit more now and can't wait for the season to start.\" \"City complete their three-match tour of the United States against Tottenham in Nashville on Saturday. Chelsea manager Antonio Conte said earlier this week he did not feel Tottenham were judged by the same standards as his own side, City and Manchester United. Spurs have had the advantage in their recent meetings with City, winning three and drawing one of their last four Premier League games. And Gundogan thinks they are a major threat.\"\nprompt = f\"Summarize the following text:\\n{en_context}\"\ninputs = tokenizer(prompt, max_length=256, return_tensors=\"pt\").to(\"cuda\")\noutput = model.generate(**inputs, do_sample=False)\noutput = tokenizer.batch_decode(output, skip_special_tokens=True)\nprint(f\"Text: \\n{prompt}\\n---------------\")\nprint(f\"Summary: \\n{output}\\n---------------\")","metadata":{"execution":{"iopub.status.busy":"2024-02-20T12:58:48.663569Z","iopub.execute_input":"2024-02-20T12:58:48.663924Z","iopub.status.idle":"2024-02-20T12:58:49.107381Z","shell.execute_reply.started":"2024-02-20T12:58:48.663894Z","shell.execute_reply":"2024-02-20T12:58:49.106406Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Text: \nSummarize the following text:\n Gundogan, 26, told BBC Sport he can see the finishing line after tearing cruciate knee ligaments in December, but will not rush his return. The German missed the 2014 World Cup following back surgery that kept him out for a year, and sat out Euro 2016 because of a dislocated kneecap. He said that it is heavy mentally to accept that. Gundogan will not be fit for the start of the Premier League season at Brighton on 12 August but said his recovery time is now being measured in weeks rather than months. Gundogan made 15 appearances and scored five goals in his debut season for City following his £20m move from Borussia Dortmund. He is eager to get on the field again and was impressed at the club's 4-1 win over Real Madrid in a pre-season game in Los Angeles on Wednesday. Manager Pep Guardiola has made five new signings already this summer and continues to have an interest in Arsenal forward Alexis Sanchez and Monaco's Kylian Mbappe. Gundogan said that we felt that last year as well but it was a completely new experience for all of us. We know the Premier League a bit more now and can't wait for the season to start.City complete their three-match tour of the United States against Tottenham in Nashville on Saturday. Chelsea manager Antonio Conte said earlier this week he did not feel Tottenham were judged by the same standards as his own side, City and Manchester United. Spurs have had the advantage in their recent meetings with City, winning three and drawing one of their last four Premier League games. And Gundogan thinks they are a major threat.\n---------------\nSummary: \n['The German missed the 2014 World Cup following back surgery that kept him out for']\n---------------\n","output_type":"stream"}]},{"cell_type":"code","source":"# Swahili example\nswa_text = swa_sample[\"text\"]\nprompt = f\"Summarize the following text:\\n{swa_text}\"\ninputs = tokenizer(prompt, max_length=256, return_tensors=\"pt\").to(\"cuda\")\noutput = model.generate(**inputs, do_sample=False, max_new_tokens=64)\noutput = tokenizer.batch_decode(output, skip_special_tokens=True)\nprint(f\"Text: \\n{prompt}\\n---------------\")\nprint(f\"Summary: \\n{output}\\n---------------\")","metadata":{"execution":{"iopub.status.busy":"2024-02-20T13:01:46.529633Z","iopub.execute_input":"2024-02-20T13:01:46.530596Z","iopub.status.idle":"2024-02-20T13:01:46.991747Z","shell.execute_reply.started":"2024-02-20T13:01:46.530561Z","shell.execute_reply":"2024-02-20T13:01:46.990685Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Text: \nSummarize the following text:\n Magari zaidi ya 50 aina ya Toyota Prado hayajapatikana Chama tawala kilihesabu magari hayo mwezi mmoja kabla ya kuingia madarakani baada ya kupata ushindi uchaguzini mwezi Desemba. Imekuwa kawaida kwa baadhi ya maafisa wa serikali inayoondoka kutorejesha magari ya serikali, na hulazimu serikali mpya kuyatwaa kwa nguvu nchini Ghama. Waziri mmoja katika serikali iliyoondoka ya John Mahama hata hivyo amesema kuenezwa kwa habari kwamba wenzake walitekeleza uhalifu ni makosa. Aliyekuwa waziri wa usalama Omane Boamah ameambia mwandishi wa BBC Thomas Naadi kwamba hiyo ni \"mbinu inayotumiwa na serikali mpya kuipa sababu za kununua magari mapya.\" Msemaji wa rais Eugene Arhin aliambia wanahabari kwamba maafisa wa serikali mpya walipata magari: Kituo cha redio cha Citi FM nchini Ghana kimeripoti kwamba rais amelazimika kutumia gari aina ya BMW lililoundwa miaka 10 iliyopita kutokana na kutorejeshwa kwa magari hayo. Nana Akufo-Addo (kulia) alimshinda John Mahama (kushoto) mwezi Desemba Alipokuwa anatoa taarifa yake, Bw Arhin alifichua kwamba afisi ya rais ilifaa kuwa na magari zaidi ya 300 lakini hakueleza magari hayo hutumiwa vipi. Nana Akufo-Addo wa chama cha New Patriotic Party alishinda uchaguzi mkuu mwanzoni mwa Desemba na kuchukua hatamu kutoka kwa John Mahama, wa chama cha National Democratic Congress.\n---------------\nSummary: \n['The following text is a paragraph about the following text: Magari zaidi']\n---------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Q2: How beneficial is the instruction dataset in the target language?","metadata":{}},{"cell_type":"code","source":"# Our Swahili instruction dataset\n# This dataset is translated from Dolly-15k English instructions, later filtered and post-edited by Toloka\n!wget https://github.com/AligningLLMtoLRL/AligningLLMtoLRL.github.io/raw/main/materials/Dataset.zip\n!unzip Dataset.zip","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndolly_swahili_df = pd.read_excel(\"/kaggle/working/translated_ds.xlsx\")\ndolly_swahili_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T04:31:41.412162Z","iopub.execute_input":"2024-02-21T04:31:41.412541Z","iopub.status.idle":"2024-02-21T04:31:45.886605Z","shell.execute_reply.started":"2024-02-21T04:31:41.412514Z","shell.execute_reply":"2024-02-21T04:31:45.885589Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                                task_id  \\\n0  000287b55d--656f562fa7ccfa2fa62cbad5   \n1  000287b55d--656f562fa7ccfa2fa62cbb0b   \n\n                                    INPUT:context_tr  \\\n0   \"I'm So Excited\" ni wimbo wa mwimbaji wa Aust...   \n1                                                      \n\n                                   INPUT:context_src  \\\n0  \"I'm So Excited\" is a song by Australian singe...   \n1                                                      \n\n                                   INPUT:response_tr  \\\n0  \"I'm So Excited\" ni wimbo wa mwimbaji wa Austr...   \n1  Kupanga safari ya kwenda Ulaya ni sawa na kupa...   \n\n                                  INPUT:response_src  \\\n0  \"I'm So Excited\" is a song by Australian singe...   \n1  Planning a trip to Europe is similar to planni...   \n\n                                INPUT:instruction_tr  \\\n0       Ni nani mwimbaji wa wimbo wa I'm So Excited?   \n1   Je, nifanyeje kuhusu kupanga safari ya kwenda...   \n\n                              INPUT:instruction_src  toloka probabilities  \n0     Who is the singer of the song I'm So Excited?              0.988446  \n1  How should I go about planning a trip to Europe?              0.982769  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>task_id</th>\n      <th>INPUT:context_tr</th>\n      <th>INPUT:context_src</th>\n      <th>INPUT:response_tr</th>\n      <th>INPUT:response_src</th>\n      <th>INPUT:instruction_tr</th>\n      <th>INPUT:instruction_src</th>\n      <th>toloka probabilities</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000287b55d--656f562fa7ccfa2fa62cbad5</td>\n      <td>\"I'm So Excited\" ni wimbo wa mwimbaji wa Aust...</td>\n      <td>\"I'm So Excited\" is a song by Australian singe...</td>\n      <td>\"I'm So Excited\" ni wimbo wa mwimbaji wa Austr...</td>\n      <td>\"I'm So Excited\" is a song by Australian singe...</td>\n      <td>Ni nani mwimbaji wa wimbo wa I'm So Excited?</td>\n      <td>Who is the singer of the song I'm So Excited?</td>\n      <td>0.988446</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000287b55d--656f562fa7ccfa2fa62cbb0b</td>\n      <td></td>\n      <td></td>\n      <td>Kupanga safari ya kwenda Ulaya ni sawa na kupa...</td>\n      <td>Planning a trip to Europe is similar to planni...</td>\n      <td>Je, nifanyeje kuhusu kupanga safari ya kwenda...</td>\n      <td>How should I go about planning a trip to Europe?</td>\n      <td>0.982769</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import Dataset\n\n# Load our Swahili instruction dataset\n# This dataset is translated from Dolly-15k English instructions, later filtered and post-edited\ndolly_swahili = Dataset.from_pandas(dolly_swahili_df)\n\n# Modify dataset to make consistent with original dolly\ndolly_swahili = dolly_swahili.rename_column(\"INPUT:context_tr\", \"context\")\ndolly_swahili = dolly_swahili.rename_column(\"INPUT:instruction_tr\", \"instruction\")\ndolly_swahili = dolly_swahili.rename_column(\"INPUT:response_tr\", \"response\")\n\n# Preprocess the dataset\ntokenized_dolly_swahili = dolly_swahili.map(preprocess_dolly, batched=True,\n                                           remove_columns=[\"INPUT:context_src\", \"INPUT:instruction_src\", \"INPUT:response_src\", \"toloka probabilities\", \"task_id\"]))","metadata":{"execution":{"iopub.status.busy":"2024-02-21T04:31:53.982034Z","iopub.execute_input":"2024-02-21T04:31:53.983368Z","iopub.status.idle":"2024-02-21T04:32:01.527573Z","shell.execute_reply.started":"2024-02-21T04:31:53.983336Z","shell.execute_reply":"2024-02-21T04:32:01.526651Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12125 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bf54c181db64bdba01afc93aea328e4"}},"metadata":{}}]},{"cell_type":"code","source":"# load mT5 from the hub for a new fine-tuning\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n\ntrain_batch_size = 2\neval_batch_size = 8\n\n# Hugging Face repository id\nswa_model_id = f\"mt5_base_swa_dolly\"\n\n# Create Trainer instance\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=Seq2SeqTrainingArguments(\n        output_dir=swa_model_id,\n        per_device_train_batch_size=train_batch_size,\n        per_device_eval_batch_size=eval_batch_size,\n        gradient_accumulation_steps=4,\n        predict_with_generate=True,\n        #fp16=True, # Overflows with fp16\n        learning_rate=1e-3,\n        max_steps=1000,\n        # logging & evaluation strategies\n        logging_strategy=\"steps\",\n        logging_steps=250,\n        evaluation_strategy=\"no\",\n        save_strategy=\"no\",\n        load_best_model_at_end=True,\n        generation_max_length=max_target_len,\n        report_to=\"none\",\n        push_to_hub=False,\n    ),\n    data_collator=data_collator,","metadata":{"execution":{"iopub.status.busy":"2024-02-21T04:37:30.800390Z","iopub.execute_input":"2024-02-21T04:37:30.800857Z","iopub.status.idle":"2024-02-21T04:37:37.744306Z","shell.execute_reply.started":"2024-02-21T04:37:30.800819Z","shell.execute_reply":"2024-02-21T04:37:37.743412Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Start training\ntrainer.train()\n\n# Save the model\ntrainer.save_model()","metadata":{"execution":{"iopub.status.busy":"2024-02-21T04:37:41.960483Z","iopub.execute_input":"2024-02-21T04:37:41.961391Z","iopub.status.idle":"2024-02-21T04:52:34.115957Z","shell.execute_reply.started":"2024-02-21T04:37:41.961356Z","shell.execute_reply":"2024-02-21T04:52:34.115110Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 14:46, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>250</td>\n      <td>5.913700</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>3.824100</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>3.567600</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>3.346000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluate the fine-tuned model\ntrainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-02-20T18:43:19.287326Z","iopub.execute_input":"2024-02-20T18:43:19.288208Z","iopub.status.idle":"2024-02-20T18:43:38.521253Z","shell.execute_reply.started":"2024-02-20T18:43:19.288174Z","shell.execute_reply":"2024-02-20T18:43:38.520018Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7/7 00:14]\n    </div>\n    "},"metadata":{}},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 3.438464641571045,\n 'eval_rouge1': 18.7487,\n 'eval_rouge2': 5.0003,\n 'eval_rougeL': 15.0974,\n 'eval_rougeLsum': 15.1395,\n 'eval_gen_len': 35.81818181818182,\n 'eval_runtime': 19.2231,\n 'eval_samples_per_second': 5.15,\n 'eval_steps_per_second': 0.364}"},"metadata":{}}]},{"cell_type":"code","source":"# Swahili example\nswa_text = swa_sample[\"text\"]\nprompt = f\"Summarize the following text:\\n{swa_text}\"\ninputs = tokenizer(prompt, max_length=256, return_tensors=\"pt\").to(\"cuda\")\noutput = model.generate(**inputs, do_sample=False, max_new_tokens=64)\noutput = tokenizer.batch_decode(output, skip_special_tokens=True)\nprint(f\"Text: \\n{prompt}\\n---------------\")\nprint(f\"Summary: \\n{output}\\n---------------\")","metadata":{"execution":{"iopub.status.busy":"2024-02-20T13:49:23.326780Z","iopub.execute_input":"2024-02-20T13:49:23.328120Z","iopub.status.idle":"2024-02-20T13:49:25.482760Z","shell.execute_reply.started":"2024-02-20T13:49:23.328086Z","shell.execute_reply":"2024-02-20T13:49:25.481613Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Text: \nSummarize the following text:\n Magari zaidi ya 50 aina ya Toyota Prado hayajapatikana Chama tawala kilihesabu magari hayo mwezi mmoja kabla ya kuingia madarakani baada ya kupata ushindi uchaguzini mwezi Desemba. Imekuwa kawaida kwa baadhi ya maafisa wa serikali inayoondoka kutorejesha magari ya serikali, na hulazimu serikali mpya kuyatwaa kwa nguvu nchini Ghama. Waziri mmoja katika serikali iliyoondoka ya John Mahama hata hivyo amesema kuenezwa kwa habari kwamba wenzake walitekeleza uhalifu ni makosa. Aliyekuwa waziri wa usalama Omane Boamah ameambia mwandishi wa BBC Thomas Naadi kwamba hiyo ni \"mbinu inayotumiwa na serikali mpya kuipa sababu za kununua magari mapya.\" Msemaji wa rais Eugene Arhin aliambia wanahabari kwamba maafisa wa serikali mpya walipata magari: Kituo cha redio cha Citi FM nchini Ghana kimeripoti kwamba rais amelazimika kutumia gari aina ya BMW lililoundwa miaka 10 iliyopita kutokana na kutorejeshwa kwa magari hayo. Nana Akufo-Addo (kulia) alimshinda John Mahama (kushoto) mwezi Desemba Alipokuwa anatoa taarifa yake, Bw Arhin alifichua kwamba afisi ya rais ilifaa kuwa na magari zaidi ya 300 lakini hakueleza magari hayo hutumiwa vipi. Nana Akufo-Addo wa chama cha New Patriotic Party alishinda uchaguzi mkuu mwanzoni mwa Desemba na kuchukua hatamu kutoka kwa John Mahama, wa chama cha National Democratic Congress.\n---------------\nSummary: \n['Waziri mmoja katika serikali iliyoondoka na ushindi wa serikali']\n---------------\n","output_type":"stream"}]}]}